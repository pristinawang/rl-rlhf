# rl-rlhf

# To run PPO
Please refer to README and use requirements.txt to create env.
Then run ppo.py

# To run DPO
Please refer to DPO-LoRA.ipynb
